{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome \n",
    "\n",
    "Notebooks by BlazingSQL is a high-performance Jupyter lab experience. Utilizing CUDA GPUs and the RAPIDS AI ecosystem, you can scale your data science to tackle enterprise scale challenges in an easy to share, reproducable environment.\n",
    "\n",
    "[What is BlazingSQL?](https://medium.com/future-vision/what-is-blazingsql-33022a677dee?source=friends_link&sk=48619d5ee63ad82e0a22d244d4bcf43a)\n",
    "\n",
    "<details><summary>Instance details...</summary>\n",
    "\n",
    "Every Notebooks environment has:\n",
    "    \n",
    "- An allocated CUDA GPU\n",
    "    - Optional access to multi-GPU servers (COMING SOON - [join waitlist](https://google.com)) \n",
    "- Pre-Installed GPU Data Science Packages \n",
    "    - BlazingSQL, Dask, PyTorch, RAPIDS, & more\n",
    "- Anaconda access to all of your favorite libraries & packages \n",
    "\n",
    "Our vision is to create an affordably scaleable workspace for data science that is incredibly performant and opens the door to optimized workflows. Thank you for your participation in this Alpha version. Check out our [introductory video](https://youtu.be/yqFa4_--dMQ), or start running GPU-accelerated code below!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlazingContext API\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "    \n",
    "BlazingContext is the Python API of BlazingSQL. \n",
    "    \n",
    "Initializing BlazingContext connects allows you to create tables, run queries and utilize the power of GPU accelerated SQL.\n",
    "\n",
    "[GitHub](https://github.com/BlazingDB/blazingsql) | [Intro Notebook](intro_notebooks/blazingcontext.ipynb)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazingsql import BlazingContext\n",
    "\n",
    "# connect to BlazingSQL w/ BlazingContext API\n",
    "bc = BlazingContext(pool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# identify full file path to sample_taxi.csv\n",
    "import os\n",
    "data_path = f'{os.getcwd()}/data/sample_taxi.csv'\n",
    "\n",
    "# create a BlazingSQL table from any file w/ .create_table(table_name, file_path)\n",
    "bc.create_table('taxi', data_path, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about [creating](intro_notebooks/create_tables.ipynb) and [querying](intro_notebooks/query_tables.ipynb) BlazingSQL tables, or the [BlazingContext API](intro_notebooks/blazingcontext.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handoff Results\n",
    "\n",
    "BlazingContext returns each query's results as a cuDF DataFrame, making for easy handoff to GPU or non-GPU solutions.\n",
    "\n",
    "#### cuDF - GPU DataFrame Library\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "\n",
    "Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n",
    "\n",
    "cuDF provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.\n",
    "    \n",
    "[GitHub](https://github.com/rapidsai/cudf) | [Intro Notebook](intro_notebooks/bsql_cudf.ipynb)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bc.sql('select * from taxi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell me about this taxi data\n",
    "bc.sql('select * from taxi').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about [BlazingSQL + cuDF](intro_notebooks/bsql_cudf.ipynb) BlazingSQL tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Vizualization\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "from colorcet import fire\n",
    "\n",
    "# execute query & lay out a canvas w/ dropoff locations \n",
    "nyc = ds.Canvas().points(bc.sql('SELECT dropoff_x, dropoff_y FROM taxi'), 'dropoff_x', 'dropoff_y')\n",
    "\n",
    "# shade in the picture w/ fire & display\n",
    "ds.transfer_functions.set_background(ds.transfer_functions.shade(nyc, cmap=fire), \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuML - RAPIDS Machine Learning Library\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "\n",
    "cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\n",
    "\n",
    "cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches the API from scikit-learn.\n",
    "\n",
    "For large datasets, these GPU-based implementations can complete 10-50x faster than their CPU equivalents. For details on performance, see the cuML Benchmarks Notebook.\n",
    "    \n",
    "[GitHub](https://github.com/rapidsai/cuml) | [Intro Notebook](intro_notebooks/cuml.ipynb)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cuml import LinearRegression\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "\n",
    "# pull feature (X) and target (y) values\n",
    "X = bc.sql('SELECT trip_distance, tolls_amount FROM taxi')\n",
    "y = bc.sql('SELECT fare_amount FROM taxi')['fare_amount']\n",
    "\n",
    "# split data into train and test sets (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# call Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test X values\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# convert test & predicted values to pandas series for r2_score\n",
    "r2_score(y_true=y_test.to_pandas(), y_pred=y_pred.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuSignal - GPU-Accelerated Signal Processing\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "\n",
    "The RAPIDS cuSignal project leverages CuPy, Numba, and the RAPIDS ecosystem for GPU accelerated signal processing. \n",
    "    \n",
    "In some cases, cuSignal is a direct port of Scipy Signal to leverage GPU compute resources via CuPy but also contains Numba CUDA kernels for additional speedups for selected functions. \n",
    "    \n",
    "cuSignal achieves its best gains on large signals and compute intensive functions but stresses online processing with zero-copy memory (pinned, mapped) between CPU and GPU.\n",
    "\n",
    "[GitHub](https://github.com/rapidsai/cusignal) | [Intro Notebook](intro_notebooks/cusignal.ipynb)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cusignal\n",
    "import cupy as cp\n",
    "\n",
    "start = 0\n",
    "stop = 10\n",
    "num_samps = int(1e8)\n",
    "resample_up = 2\n",
    "resample_down = 3\n",
    "\n",
    "gx = cp.linspace(start, stop, num_samps, endpoint=False) \n",
    "gy = cp.cos(-gx**2/6.0)\n",
    "\n",
    "gf = cusignal.resample_poly(gy, resample_up, resample_down, window=('kaiser', 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cuGraph - RAPIDS Graph Analytics Library\n",
    "\n",
    "Pending resolution of [rapidsai/cugraph#744](https://github.com/rapidsai/cugraph/issues/744).\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "\n",
    "The RAPIDS cuGraph library is a collection of graph analytics that process data found in GPU Dataframes - see cuDF.\n",
    "\n",
    "cuGraph aims to provide a NetworkX-like API that will be familiar to data scientists, so they can now build GPU-accelerated workflows more easily.\n",
    "    \n",
    "[GitHub](https://github.com/rapidsai/cugraph) | [Intro Notebook](intro_notebooks/cugraph.ipynb)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cugraph\n",
    "\n",
    "# # assuming that data has been loaded into a cuDF (using read_csv) Dataframe\n",
    "# # gdf = cudf.read_csv(\"graph_data.csv\", names=[\"src\", \"dst\"], dtype=[\"int32\", \"int32\"] )\n",
    "# bc.create_table('karate', f'{os.getcwd()}/data/karate.csv', names=[\"src\", \"dst\"])\n",
    "# # gdf = bc.sql('SELECT fare_amount, tip_amount FROM taxi')\n",
    "# gdf = bc.sql('select src, dst from karate')\n",
    "# # create a Graph using the source (src) and destination (dst) vertex pairs the GDF  \n",
    "# G = cugraph.Graph()\n",
    "# # G.add_edge_list(gdf, source='src', destination='dst')\n",
    "# # G.add_edge_list(gdf, source='fare_amount', destination='tip_amount')\n",
    "\n",
    "# # Call cugraph.pagerank to get the pagerank scores\n",
    "# gdf_page = cugraph.pagerank(G)\n",
    "\n",
    "# for i in range(len(gdf_page)):\n",
    "#     print(\"vertex \" + str(gdf_page['vertex'][i]) + \" PageRank is \" + str(gdf_page['pagerank'][i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage Plugins - Scale Your Data\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "    \n",
    "We think you should let data rest wherever it likes. Don't worry about synching, directly query files wherever they reside.\n",
    "\n",
    "With the BlazingSQL Filesystem API, you can register and connect to multiple storage solutions. \n",
    "\n",
    "- [AWS](https://docs.blazingdb.com/docs/s3) \n",
    "- [Google Storage](https://docs.blazingdb.com/docs/google-cloud-storage)\n",
    "- [HDFS](https://docs.blazingdb.com/docs/hdfs)\n",
    "\n",
    "Once a filesystem is registered you can reference the user-defined file path when creating a new table off of a file.\n",
    "    \n",
    "[Docs](https://docs.blazingdb.com/docs/connecting-data-sources) | [Intro notebook](intro_notebooks/storage_plugins.ipynb)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register an AWS S3 bucket\n",
    "bc.s3('bsql_data', bucket_name='blazingsql-colab', access_key_id='', secret_key='')\n",
    "\n",
    "# create table from S3 bucket\n",
    "bc.create_table('orders', 's3://bsql_data/tpch_sf1/orders/0_0_0.parquet')\n",
    "\n",
    "# query table from S3 bucket\n",
    "bc.sql('select * from orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlazingSQL Logs\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "    \n",
    "BlazingSQL has an internal log that records events from every node from all queries run. The events include runtime query step execution information, performance timings, errors and warnings. \n",
    "\n",
    "The logs table is called `bsql_logs`. You can query the logs as if it were any other table, except you use the `.log()` function, instead of the `.sql()` function.\n",
    "    \n",
    "[Docs](https://docs.blazingdb.com/docs/blazingsql-logs) | [Intro Notebook](intro_notebooks/bsql_logs.ipynb)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long did each successfully run query take?\n",
    "bc.log(\"SELECT log_time, query_id, duration FROM bsql_logs WHERE info = 'Query Execution Done' ORDER BY log_time DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cyber Log Accelerators \n",
    "\n",
    "RAPIDS Cyber Log Accelerators (CLX)\n",
    "\n",
    "<details><summary>Learn more...</summary>\n",
    "    \n",
    "CLX (\"clicks\") provides a collection of RAPIDS examples for security analysts, data scientists, and engineers to quickly get started applying RAPIDS and GPU acceleration to real-world cybersecurity use cases.\n",
    "\n",
    "The goal of CLX is to:\n",
    "\n",
    "- Allow cyber data scientists and SecOps teams to generate workflows, using cyber-specific GPU-accelerated primitives and methods, that let them interact with code using security language,\n",
    "- Make available pre-built use cases that demonstrate CLX and RAPIDS functionality that are ready to use in a Security Operations Center (SOC),\n",
    "- Accelerate log parsing in a flexible, non-regex method. and\n",
    "- Provide SIEM integration with GPU compute environments via RAPIDS and effectively extend the SIEM environment.\n",
    "    \n",
    "[GitHub](https://github.com/rapidsai/clx) | [Intro Notebook](intro_notebooks/clx.ipynb)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import s3fs\n",
    "from os import path\n",
    "\n",
    "# download data\n",
    "if not path.exists(\"./splunk_faker_raw4\"):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(\"rapidsai-data/cyber/clx/splunk_faker_raw4\", \"./splunk_faker_raw4\")\n",
    "\n",
    "# read in alert data\n",
    "gdf = cudf.read_csv('./splunk_faker_raw4')\n",
    "gdf.columns = ['raw']\n",
    "\n",
    "# parse the alert data using CLX built-in parsers\n",
    "from clx.parsers.splunk_notable_parser import SplunkNotableParser\n",
    "\n",
    "snp = SplunkNotableParser()\n",
    "parsed_gdf = cudf.DataFrame()\n",
    "parsed_gdf = snp.parse(gdf, 'raw')\n",
    "\n",
    "# define function to round time to the day\n",
    "def round2day(epoch_time):\n",
    "    return int(epoch_time/86400)*86400\n",
    "\n",
    "# aggregate alerts by day\n",
    "parsed_gdf['time'] = parsed_gdf['time'].astype(int)\n",
    "parsed_gdf['day'] = parsed_gdf.time.applymap(round2day)\n",
    "day_rule_gdf= parsed_gdf[['search_name','day','time']].groupby(['search_name', 'day']).count().reset_index()\n",
    "day_rule_gdf.columns = ['rule', 'day', 'count']\n",
    "\n",
    "# import the rolling z-score function from CLX statistics\n",
    "from clx.analytics.stats import rzscore\n",
    "\n",
    "# pivot the alert data so each rule is a column\n",
    "def pivot_table(gdf, index_col, piv_col, v_col):\n",
    "    index_list = gdf[index_col].unique()\n",
    "    piv_gdf = cudf.DataFrame()\n",
    "    piv_gdf[index_col] = index_list\n",
    "    for group in gdf[piv_col].unique():\n",
    "        temp_df = gdf[gdf[piv_col] == group]\n",
    "        temp_df = temp_df[[index_col, v_col]]\n",
    "        temp_df.columns = [index_col, group]\n",
    "        piv_gdf = piv_gdf.merge(temp_df, on=[index_col], how='left')\n",
    "    piv_gdf = piv_gdf.set_index(index_col)\n",
    "    return piv_gdf.sort_index()\n",
    "\n",
    "alerts_per_day_piv = pivot_table(day_rule_gdf, 'day', 'rule', 'count').fillna(0)\n",
    "\n",
    "# create a new cuDF with the rolling z-score values calculated\n",
    "r_zscores = cudf.DataFrame()\n",
    "for rule in alerts_per_day_piv.columns:\n",
    "    x = alerts_per_day_piv[rule]\n",
    "    r_zscores[rule] = rzscore(x, 7) #7 day window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDS Stable",
   "language": "python",
   "name": "rapids-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
